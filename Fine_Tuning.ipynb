{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-Tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hongbi-Kim/Writing_lyrics/blob/master/Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onqPvDfPYxxu",
        "outputId": "1f8019cc-705f-4b16-f597-0a9937ffa2aa"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/ai-networkkr/ai-%EB%AA%A8%EB%8D%B8-%ED%83%90%ED%97%98%EA%B8%B0-3-%EB%AA%A8%EB%8D%B8-fine-tuning-feat-teachable-nlp-557677764abf"
      ],
      "metadata": {
        "id": "rVOUJn2q5SL8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QV0Lk_lMjlnC",
        "outputId": "52084c8a-5068-4fa1-aeff-5ced77e271e4"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 39.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 54.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 45.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-kUH3rfN_Bp"
      },
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "\n",
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, \n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator\n",
        "\n",
        "def train(train_file_path,model_name,\n",
        "          output_dir,\n",
        "          overwrite_output_dir,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs\n",
        "          #save_steps\n",
        "          ):\n",
        "  #tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "  tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name,\n",
        "                                                      bos_token = '</s>', eos_token = '</s>', unk_token='<unk>',\n",
        "                                                      pad_token = '<pad>', mask_token = '<mask>')\n",
        "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "      \n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "  model.save_pretrained(output_dir)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "      )\n",
        "\n",
        "  trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "  )\n",
        "      \n",
        "  trainer.train()\n",
        "  trainer.save_model()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rPEyzeB5ii_",
        "outputId": "5ee58e38-7b57-468f-884d-d502c2f6c883"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRKxMEUuCaxE"
      },
      "source": [
        "train_file_path = '/content/drive/MyDrive/Colab Notebooks/writing_lyrics/dance_lyric.txt'\n",
        "model_name = 'gpt2'\n",
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/writing_lyrics/result'\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 8\n",
        "num_train_epochs = 5.0\n",
        "save_steps = 500"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6u1ETfBxZnLx",
        "outputId": "df83a2c5-7ecb-4f74-a55b-178d734c97cd"
      },
      "source": [
        "train(\n",
        "    train_file_path=train_file_path,\n",
        "    model_name=model_name,\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=overwrite_output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs\n",
        "    #save_steps=save_steps\n",
        ")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "Loading features from cached file /content/drive/MyDrive/Colab Notebooks/writing_lyrics/cached_lm_PreTrainedTokenizerFast_128_dance_lyric.txt [took 0.048 s]\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/special_tokens_map.json\n",
            "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/pytorch_model.bin\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 10789\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 6745\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6745' max='6745' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6745/6745 21:17, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.070800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.839300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.723500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.662400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.614000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.566300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>1.539000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.529500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>1.500600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>1.476700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>1.472400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>1.453600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>1.453400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-500\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-500/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-1000\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-1000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-1000/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-1500\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-1500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-1500/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-2000\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-2000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-2000/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-2500\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-2500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-2500/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-3000\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-3000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-3000/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-3500\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-3500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-3500/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-4000\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-4000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-4000/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-4500\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-4500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-4500/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-5000\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-5000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-5000/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-5500\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-5500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-5500/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-6000\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-6000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-6000/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-6500\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-6500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-6500/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer\n",
        "\n",
        "def load_model(model_path):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def generate_text(sequence, max_length):\n",
        "    model_path = '/content/drive/MyDrive/Colab Notebooks/writing_lyrics/result'\n",
        "    model = load_model(model_path)\n",
        "    tokenizer = load_tokenizer(model_path)\n",
        "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
        "    final_outputs = model.generate(\n",
        "        ids,\n",
        "        do_sample=True,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=model.config.eos_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "odoPuqgH6nzV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = \"자세히 보아야\\n 예쁘다\\n 오래 보아야\\n 사랑스럽다\\n 너도\\n 그렇다\""
      ],
      "metadata": {
        "id": "5BaIU3Gh7AX-"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(data1, 1000)"
      ],
      "metadata": {
        "id": "wQFaPBoa6xgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a45304af-798f-44f4-e49d-63b465f4f412"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/added_tokens.json. We won't load it.\n",
            "loading file /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/vocab.json\n",
            "loading file /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/merges.txt\n",
            "loading file None\n",
            "loading file /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/special_tokens_map.json\n",
            "loading file /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/tokenizer_config.json\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
            "The class this function is called from is 'GPT2Tokenizer'.\n",
            "Adding </s> to the vocabulary\n",
            "Adding <pad> to the vocabulary\n",
            "Adding <mask> to the vocabulary\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "자세히 보아야\n",
            " 예쁘다\n",
            " 오래 보아야\n",
            " 사랑스럽다\n",
            " 너도\n",
            " 그렇다 할 수 있게 말해줘,I got your back,넌 짓데 날 맘을 원해,나의 세것은 다 나의 꿈게,매일 때 따뜻한 너와,넌 깨어 안 함께해줘\n",
            "다시 믿었으면 함께해 줘,정말 없어줘,오래오래 나도 안 볼 때면서야,모두 번져 바래 잠시 두근 같아서야,안 볼 때면서야,가슴 저기 두근 그리지 했던 나에게 함께해,나 어쩜들 모르게 사랑의 예쁘다,향해 날 안 한번 노래도 안 볼 때면서야,어딜 오려오래 나 함께해줘 네 안 볼 때면서야,내 긴 맘을 원해줘 시간에 가슴 저기 두근,네가 감추는 그리워을 너와,정말없어줘\n",
            "돌아직 나의 시간이 싶어 말해줘,나의 위한 기분이 나에게,넌 짓데 나의 세것은 다 나의 꿈게\n",
            "한 번져 바래 두근 안 함께해줘\n",
            "사랑의 한 자신게,안 볼 때 따뜻한 너와,자신게,깨어안 함께해줘,나의 위한 기분이 나에게,사랑의 함께해줘\n",
            "여행을 할 번져 바래자 나의 시간이 싶어 봐,한 번져 바래자 나의 시간이 싶어,한 번져 바래자 �\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrain된 토크나이저 사용하면 -> 20분\n",
        "# GPToken 사용하면 -> 반나절\n",
        "#tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "#tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name,"
      ],
      "metadata": {
        "id": "2lIkPO3hs5bP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}