{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-Tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hongbi-Kim/Writing_lyrics/blob/master/Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onqPvDfPYxxu",
        "outputId": "1f8019cc-705f-4b16-f597-0a9937ffa2aa"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/ai-networkkr/ai-%EB%AA%A8%EB%8D%B8-%ED%83%90%ED%97%98%EA%B8%B0-3-%EB%AA%A8%EB%8D%B8-fine-tuning-feat-teachable-nlp-557677764abf"
      ],
      "metadata": {
        "id": "rVOUJn2q5SL8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QV0Lk_lMjlnC",
        "outputId": "52084c8a-5068-4fa1-aeff-5ced77e271e4"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.0 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 596 kB 39.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 895 kB 54.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.5 MB 45.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-kUH3rfN_Bp"
      },
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "\n",
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, \n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator\n",
        "\n",
        "def train(train_file_path,model_name,\n",
        "          output_dir,\n",
        "          overwrite_output_dir,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs\n",
        "          #save_steps\n",
        "          ):\n",
        "  #tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "  tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name,\n",
        "                                                      bos_token = '</s>', eos_token = '</s>', unk_token='<unk>',\n",
        "                                                      pad_token = '<pad>', mask_token = '<mask>')\n",
        "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "      \n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "  model.save_pretrained(output_dir)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "      )\n",
        "\n",
        "  trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "  )\n",
        "      \n",
        "  trainer.train()\n",
        "  trainer.save_model()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rPEyzeB5ii_",
        "outputId": "5ee58e38-7b57-468f-884d-d502c2f6c883"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRKxMEUuCaxE"
      },
      "source": [
        "train_file_path = '/content/drive/MyDrive/Colab Notebooks/writing_lyrics/dance_lyric.txt'\n",
        "model_name = 'gpt2'\n",
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/writing_lyrics/result'\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 8\n",
        "num_train_epochs = 5.0\n",
        "save_steps = 500"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6u1ETfBxZnLx",
        "outputId": "df83a2c5-7ecb-4f74-a55b-178d734c97cd"
      },
      "source": [
        "train(\n",
        "    train_file_path=train_file_path,\n",
        "    model_name=model_name,\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=overwrite_output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs\n",
        "    #save_steps=save_steps\n",
        ")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "Loading features from cached file /content/drive/MyDrive/Colab Notebooks/writing_lyrics/cached_lm_PreTrainedTokenizerFast_128_dance_lyric.txt [took 0.048 s]\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/special_tokens_map.json\n",
            "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/pytorch_model.bin\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 10789\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 6745\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6745' max='6745' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6745/6745 21:17, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.070800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.839300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.723500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.662400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.614000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.566300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>1.539000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.529500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>1.500600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>1.476700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>1.472400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>1.453600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>1.453400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-500\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-500/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-1000\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-1000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-1000/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-1500\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-1500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-1500/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-2000\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-2000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-2000/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-2500\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-2500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-2500/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-3000\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-3000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-3000/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-3500\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-3500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-3500/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-4000\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-4000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-4000/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-4500\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-4500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-4500/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-5000\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-5000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-5000/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-5500\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-5500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-5500/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-6000\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-6000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-6000/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-6500\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-6500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/checkpoint-6500/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer\n",
        "\n",
        "def load_model(model_path):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def generate_text(sequence, max_length):\n",
        "    model_path = '/content/drive/MyDrive/Colab Notebooks/writing_lyrics/result'\n",
        "    model = load_model(model_path)\n",
        "    tokenizer = load_tokenizer(model_path)\n",
        "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
        "    final_outputs = model.generate(\n",
        "        ids,\n",
        "        do_sample=True,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=model.config.eos_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "odoPuqgH6nzV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = \"ÏûêÏÑ∏Ìûà Î≥¥ÏïÑÏïº\\n ÏòàÏÅòÎã§\\n Ïò§Îûò Î≥¥ÏïÑÏïº\\n ÏÇ¨ÎûëÏä§ÎüΩÎã§\\n ÎÑàÎèÑ\\n Í∑∏Î†áÎã§\""
      ],
      "metadata": {
        "id": "5BaIU3Gh7AX-"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(data1, 1000)"
      ],
      "metadata": {
        "id": "wQFaPBoa6xgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a45304af-798f-44f4-e49d-63b465f4f412"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/added_tokens.json. We won't load it.\n",
            "loading file /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/vocab.json\n",
            "loading file /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/merges.txt\n",
            "loading file None\n",
            "loading file /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/special_tokens_map.json\n",
            "loading file /content/drive/MyDrive/Colab Notebooks/writing_lyrics/result/tokenizer_config.json\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
            "The class this function is called from is 'GPT2Tokenizer'.\n",
            "Adding </s> to the vocabulary\n",
            "Adding <pad> to the vocabulary\n",
            "Adding <mask> to the vocabulary\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÏûêÏÑ∏Ìûà Î≥¥ÏïÑÏïº\n",
            " ÏòàÏÅòÎã§\n",
            " Ïò§Îûò Î≥¥ÏïÑÏïº\n",
            " ÏÇ¨ÎûëÏä§ÎüΩÎã§\n",
            " ÎÑàÎèÑ\n",
            " Í∑∏Î†áÎã§ Ìï† Ïàò ÏûàÍ≤å ÎßêÌï¥Ï§ò,I got your back,ÎÑå ÏßìÎç∞ ÎÇ† ÎßòÏùÑ ÏõêÌï¥,ÎÇòÏùò ÏÑ∏Í≤ÉÏùÄ Îã§ ÎÇòÏùò ÍøàÍ≤å,Îß§Ïùº Îïå Îî∞ÎúªÌïú ÎÑàÏôÄ,ÎÑå Íπ®Ïñ¥ Ïïà Ìï®ÍªòÌï¥Ï§ò\n",
            "Îã§Ïãú ÎØøÏóàÏúºÎ©¥ Ìï®ÍªòÌï¥ Ï§ò,Ï†ïÎßê ÏóÜÏñ¥Ï§ò,Ïò§ÎûòÏò§Îûò ÎÇòÎèÑ Ïïà Î≥º ÎïåÎ©¥ÏÑúÏïº,Î™®Îëê Î≤àÏ†∏ Î∞îÎûò Ïû†Ïãú ÎëêÍ∑º Í∞ôÏïÑÏÑúÏïº,Ïïà Î≥º ÎïåÎ©¥ÏÑúÏïº,Í∞ÄÏä¥ Ï†ÄÍ∏∞ ÎëêÍ∑º Í∑∏Î¶¨ÏßÄ ÌñàÎçò ÎÇòÏóêÍ≤å Ìï®ÍªòÌï¥,ÎÇò Ïñ¥Ï©úÎì§ Î™®Î•¥Í≤å ÏÇ¨ÎûëÏùò ÏòàÏÅòÎã§,Ìñ•Ìï¥ ÎÇ† Ïïà ÌïúÎ≤à ÎÖ∏ÎûòÎèÑ Ïïà Î≥º ÎïåÎ©¥ÏÑúÏïº,Ïñ¥Îîú Ïò§Î†§Ïò§Îûò ÎÇò Ìï®ÍªòÌï¥Ï§ò ÎÑ§ Ïïà Î≥º ÎïåÎ©¥ÏÑúÏïº,ÎÇ¥ Í∏¥ ÎßòÏùÑ ÏõêÌï¥Ï§ò ÏãúÍ∞ÑÏóê Í∞ÄÏä¥ Ï†ÄÍ∏∞ ÎëêÍ∑º,ÎÑ§Í∞Ä Í∞êÏ∂îÎäî Í∑∏Î¶¨ÏõåÏùÑ ÎÑàÏôÄ,Ï†ïÎßêÏóÜÏñ¥Ï§ò\n",
            "ÎèåÏïÑÏßÅ ÎÇòÏùò ÏãúÍ∞ÑÏù¥ Ïã∂Ïñ¥ ÎßêÌï¥Ï§ò,ÎÇòÏùò ÏúÑÌïú Í∏∞Î∂ÑÏù¥ ÎÇòÏóêÍ≤å,ÎÑå ÏßìÎç∞ ÎÇòÏùò ÏÑ∏Í≤ÉÏùÄ Îã§ ÎÇòÏùò ÍøàÍ≤å\n",
            "Ìïú Î≤àÏ†∏ Î∞îÎûò ÎëêÍ∑º Ïïà Ìï®ÍªòÌï¥Ï§ò\n",
            "ÏÇ¨ÎûëÏùò Ìïú ÏûêÏã†Í≤å,Ïïà Î≥º Îïå Îî∞ÎúªÌïú ÎÑàÏôÄ,ÏûêÏã†Í≤å,Íπ®Ïñ¥Ïïà Ìï®ÍªòÌï¥Ï§ò,ÎÇòÏùò ÏúÑÌïú Í∏∞Î∂ÑÏù¥ ÎÇòÏóêÍ≤å,ÏÇ¨ÎûëÏùò Ìï®ÍªòÌï¥Ï§ò\n",
            "Ïó¨ÌñâÏùÑ Ìï† Î≤àÏ†∏ Î∞îÎûòÏûê ÎÇòÏùò ÏãúÍ∞ÑÏù¥ Ïã∂Ïñ¥ Î¥ê,Ìïú Î≤àÏ†∏ Î∞îÎûòÏûê ÎÇòÏùò ÏãúÍ∞ÑÏù¥ Ïã∂Ïñ¥,Ìïú Î≤àÏ†∏ Î∞îÎûòÏûê ÔøΩ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrainÎêú ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÏÇ¨Ïö©ÌïòÎ©¥ -> 20Î∂Ñ\n",
        "# GPToken ÏÇ¨Ïö©ÌïòÎ©¥ -> Î∞òÎÇòÏ†à\n",
        "#tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "#tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name,"
      ],
      "metadata": {
        "id": "2lIkPO3hs5bP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}